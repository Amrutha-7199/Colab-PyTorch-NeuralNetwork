{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMuzEOL0V5br"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFelG9k3WU-T",
        "outputId": "720b59a8-ca02-4f86-90bb-22ffe4a0cd08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 45.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 2.06MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 12.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.07MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Load and Preprocess Data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "shuffle=True)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78d-h4C-YVJE"
      },
      "outputs": [],
      "source": [
        "#Define the CNN Model\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1,padding=1)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1,padding=1)\n",
        "    self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "    self.fc2 = nn.Linear(128, 10)\n",
        "  def forward(self, x):\n",
        "    x = self.pool(torch.relu(self.conv1(x)))\n",
        "    x = self.pool(torch.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 64 * 7 * 7)\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "model = CNN()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoJwgBPvYhOG",
        "outputId": "ac79636e-a118-4883-cbd9-0be334fe4557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.012624082594649215\n",
            "Epoch 2, Loss: 0.009749918936656358\n",
            "Epoch 3, Loss: 0.00773327993925241\n"
          ]
        }
      ],
      "source": [
        "#Train the Model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "for epoch in range(3):\n",
        "  running_loss = 0.0\n",
        "  for images, labels in trainloader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMMeFPf-Zq7C",
        "outputId": "354cbe9f-45cc-4836-bdcf-bac1fa69df6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 99.17%\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for images, labels in testloader:\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae9fM3-_aQQN"
      },
      "source": [
        "# 3. CNN Prediction & Explainability with Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "3xDG32PEaSEv",
        "outputId": "ca8d1785-5a47-4efa-f209-5a7ba09bc868"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Make a Prediction on an Image\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "index = random.randint(0, len(images) - 1)\n",
        "img = images[index].squeeze()\n",
        "true_label = labels[index].item()\n",
        "output = model(images[index].unsqueeze(0))\n",
        "predicted_label = torch.argmax(output).item()\n",
        "plt.imshow(img.numpy(), cmap='gray')\n",
        "plt.title(f\"Predicted: {predicted_label}, True: {true_label}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViUEoEvQdLIE",
        "outputId": "1a937db7-2045-4a84-c3c8-395084bc4fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini Explanation: A CNN might predict the digit 5 for an image due to a combination of factors related to its learned features and the input image's characteristics.  Here's a breakdown:\n",
            "\n",
            "* **Learned Filters and Feature Maps:**  CNNs learn hierarchical features.  Early layers might detect simple edges and curves.  As you go deeper, layers combine these simpler features into more complex ones, like loops, strokes, and junctions.  For the prediction of \"5\", the CNN likely activated filters that detect features commonly associated with this digit.  These could include:\n",
            "    * A curved top stroke.\n",
            "    * A vertical straight line.\n",
            "    * A shorter, angled bottom stroke.\n",
            "    * The specific spatial arrangement of these strokes.\n",
            "* **Activation Strengths:** The strength of activation of these filters plays a crucial role.  If the filters corresponding to \"5\" features are strongly activated, while filters related to other digits are weakly activated, the model leans towards predicting \"5\".\n",
            "* **Fully Connected Layers:** The final fully connected layers aggregate information from the convolutional layers and perform the classification. The weights in these layers are learned during training to map the activated features to specific digit classes. If the pattern of activated features strongly resembles the patterns learned for \"5\" during training, the output layer will assign a high probability to this class.\n",
            "* **Training Data:** The CNN's prediction is heavily influenced by the data it was trained on. If the training data contained numerous examples of \"5\"s written in a particular style, the model will be more sensitive to those styles.  A \"5\" written in a drastically different style might be misclassified.\n",
            "* **Image Quality and Noise:** Noise, blur, or occlusions in the image can affect feature extraction.  If crucial parts of the \"5\" are obscured, the model might struggle to activate the correct filters and could misclassify.  Conversely, even stray marks that coincidentally resemble parts of a \"5\" could lead to a false positive.\n",
            "* **Adversarial Examples:**  While less likely in a standard scenario, specifically crafted adversarial examples could fool the CNN.  These are images with subtle, often imperceptible perturbations designed to exploit vulnerabilities in the model's decision-making process, leading to incorrect classifications.\n",
            "\n",
            "In essence, the CNN's \"5\" prediction is a result of a complex interplay between its learned features, the input image's characteristics, and the strengths of activations propagating through the network.  It's not a single feature but the overall pattern of activated features that drives the final decision.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#GEMINI_API_KEY = \"AIzaSyBCM3mBviNCgko-uKldcQtJ0xvWQRYArJ8\"\n",
        "\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Replace with your actual Gemini API key\n",
        "GEMINI_API_KEY = \"AIzaSyBCM3mBviNCgko-uKldcQtJ0xvWQRYArJ8\"\n",
        "GEMINI_ENDPOINT = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent\"\n",
        "\n",
        "# Define headers\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Define the prompt\n",
        "predicted_label = 5  # Example digit\n",
        "prompt_text = f\"The CNN model predicted digit {predicted_label} for an image. Explain why it might have made this prediction.\"\n",
        "\n",
        "# Format the data as expected by Gemini API\n",
        "data = {\n",
        "    \"contents\": [\n",
        "        {\n",
        "            \"parts\": [{\"text\": prompt_text}]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Send the POST request\n",
        "response = requests.post(f\"{GEMINI_ENDPOINT}?key={GEMINI_API_KEY}\", headers=headers, json=data)\n",
        "\n",
        "# Parse the response\n",
        "response_json = response.json()\n",
        "\n",
        "# Check if response contains the expected content\n",
        "if \"candidates\" in response_json:\n",
        "    explanation = response_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    print(\"Gemini Explanation:\", explanation)\n",
        "else:\n",
        "    print(\"Error:\", response_json)  # Print full response for debugging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discuss Explainability\n",
        "o Compare explanations for correct vs. incorrect predictions.\n",
        "o Discuss how LLMs enhance model interpretability.\n",
        "o Highlight the importance of explainability in AI applications.\n",
        "\n",
        "(1)For correct results Gemini describes how the input image matches the CNN's learned features, such as curves and strokes, supporting the prediction (e.g., \"5\" has a curved top, vertical line, and angled bottom).\n",
        "For incorrect results Gemini explains the model's confusion (for example, \"3\" and \"5\" share curves, but \"3\" was preferred due to spatial layout). Additionally, it could draw attention to problems like hostile examples, noise, or low image quality.\n",
        "\n",
        "(2) Gemini and other LLMs improve model interpretability by converting technical ideas into understandable insights through context-specific, human-readable explanations. Interactive debugging is made possible for quicker refining and aids in the diagnosis of flaws like noise or biases. LLMs bridge the gap between human comprehension and sophisticated model outputs, increasing the transparency and accessibility of AI systems.\n",
        "\n",
        "(3) Explainability in AI makes decision-making processes comprehensible, ensuring transparency and fostering confidence. It assists in detecting biases, meeting regulatory requirements, and enhancing model performance by pointing out flaws. Furthermore, it gives users the ability to make knowledgeable choices in crucial applications like autonomous driving and healthcare."
      ],
      "metadata": {
        "id": "YZ8o52FeIvIn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ULgALKXfrgt"
      },
      "source": [
        "# Small CNN Task – CIFAR-10 Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tMQ13pqjO1c",
        "outputId": "77650655-865c-4e2e-c3fc-60daadc3871d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 74.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#Load and Preprocess Data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "shuffle=True)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcmWVhghaXbn"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)  # Fix: changed in_channels from 1 to 3\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate model\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EgSO8HtHhn9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce8acb4-13e1-40a7-cc52-bb36bdf92cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.708929928062517\n",
            "Epoch 2, Loss: 0.6009291391390974\n"
          ]
        }
      ],
      "source": [
        "#Train the Model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "for epoch in range(2):\n",
        "  running_loss = 0.0\n",
        "  for images, labels in trainloader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for images, labels in testloader:\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-lfvpkRFMp2",
        "outputId": "a71e6974-80d9-43a6-f213-5384f1ba7270"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 72.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare MNIST vs. CIFAR-10 Model Complexity\n",
        "Dataset complexity directly affects model design and performance, as demonstrated by the fact that MNIST, which uses simple grayscale images of digits, achieves high accuracy (99.17%) and low loss quickly, indicating that it is an easier task for models, while CIFAR-10, which uses complex color images of objects, has lower accuracy (72.27%) and higher loss, reflecting its greater complexity. MNIST requires simpler models and fewer epochs to train, while CIFAR-10 requires deeper architectures and more training time."
      ],
      "metadata": {
        "id": "i7aVcTGEMC5B"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}